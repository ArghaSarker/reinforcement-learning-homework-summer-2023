{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOugvmdz8/AFZZqKzShxslA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArghaSarker/reinforcement-learning-homework-summer-2023/blob/main/homework04/homework04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementing ATARI-game \"Breakout\" as a Deep Q-Network"
      ],
      "metadata": {
        "id": "12qJ0xUTjPRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#General GYM commands\n",
        "\n",
        "###Create (vector) environment\n",
        "`env = gym.make(\"ENVIRONMENT_NAME\")` <br>\n",
        "`envs = gym.vector.make(\"ENVIRONMENT_NAME\", N_ENVIRONMENTS)`\n",
        "\n",
        "###Reset environment to initial state\n",
        "`observation, info = env.reset()` <br>\n",
        "***NOTE: In vectorized environment, we do not have to reset the environment when it is terminated/truncated, because whenever a sub-environment finishes, it starts again automatically. This is for efficiency, because we will end at different time points for the sub-environments, and can't afford to wait until all are terminated.\n",
        "\n",
        "###Sample a random action in environment\n",
        "`action = env.action_space.sample()`\n",
        "\n",
        "###Take an action in environment\n",
        "`observation, reward, terminated, truncated, info = env.step(action)`\n",
        "- observation = new state (image)\n",
        "- reward = reward (int)\n",
        "- terminated = terminal state reached? (bool)\n",
        "- truncated = max sequence length reached? (bool)\n",
        "- info = human-readable information"
      ],
      "metadata": {
        "id": "32NRmaWyoz7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Breakout (ALE/Breakout-v5)\n",
        "\n",
        "###Action Space (`env.action_space`)\n",
        "***There are 4 different actions conducted by pressing 4 different buttoms.\n",
        "- action 0 (NOOP): perform no action\n",
        "- action 1 (FIRE): restart after losing a life\n",
        "- action 2 (RIGHT): move platform to the right\n",
        "- action 3 (LEFT): move platform to the left\n",
        "\n",
        "###Observation Space (`env.observation_space`)\n",
        "The output state / observation is an image with 210x160 pixels + 3 color channels (rgb), i.e. a vector of size **(210,160,3)**. <br>\n",
        "The observation space is a vector of size **(0,255,(210,160,3),uint8)**, i.e. each data point in this space is an image of size (210,160,3), where each pixel can take an unsigned integer (with 8 Bits) - value between [0,255].\n",
        "\n",
        "###Frameskip\n",
        "Uses a frameskip of **4**: Instead of playing each frame of the game independently, we play 4 frames at once, i.e. we do the same action for all 4 frames with a probability of 0.25."
      ],
      "metadata": {
        "id": "M0zZUmKbjW7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tool Setup"
      ],
      "metadata": {
        "id": "pVbVy2xAex62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gym environment (Atari Games): https://gymnasium.farama.org/#"
      ],
      "metadata": {
        "id": "b2cZv9a3dxGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "id": "lAUojlsxdcWY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef466324-f321-4b4e-e589-deecb1868ecb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.3.25)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.6.3)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"gymnasium[atari, accept-rom-license]\""
      ],
      "metadata": {
        "id": "dPr7wMamePXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9423c2a1-7bad-414d-a335-5bc0f2fb21ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.23.5)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.6.3)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: shimmy[atari]<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (8.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (4.65.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (0.6.1)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]) (5.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1hXmXS2yULM2"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym    #for atari games environment\n",
        "import tensorflow as tf    #for deep ANN\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import time\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experience Replay Buffer\n",
        "= stores all collected samples, i.e. works as a dataset that is used for training our Q-Network\n"
      ],
      "metadata": {
        "id": "uXT6dpwjgqao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceReplayBuffer:\n",
        "\n",
        "  def __init__(self, max_size: int, environment_name: str, parallel_game_unrolls: int, observation_preprocessing_function: callable, unroll_steps: int):\n",
        "    self.max_size = max_size                                                      #amount of max samples that can be stored\n",
        "    self.environment_name = environment_name                                      #the environment we use\n",
        "    self.parallel_game_unrolls = parallel_game_unrolls                            #amount of environments, in which we play in parallel\n",
        "    self.observation_preprocessing_function = observation_preprocessing_function  #preprocessing function used to [...] the observations\n",
        "    self.unroll_steps = unroll_steps                                              #amount of steps that we take in each sub-environment to generate data sample\n",
        "    self.envs = gym.vector.make(environment_name, self.parallel_game_unrolls)     #create vectorized environment to allow sampling from multiple envs in parallel\n",
        "    self.num_possible_actions = self.envs.single_action_space.n                   #amount of possible actions that can be taken in a sub-environment\n",
        "    self.current_states, _ = self.envs.reset()                                    #stores the current state of each sub-env\n",
        "    self.data = []                                                                #stores all tf.datasets that each represent the information of a single step at time step t in all sub-environments\n",
        "\n",
        "  def sample_epsilon_greedy(self, dqn_network, epsilon: float):\n",
        "    \"\"\" sample an action from DQN given the observation by using  epsilon greedy approach \"\"\"\n",
        "    #get observation / current state\n",
        "    observations = self.observation_preprocessing_function(self.current_states)\n",
        "    #run q-network on observation to estimate q-values\n",
        "    q_values = dqn_network(observations)                                                                                                           #tensor of type tf.float32 + shape (parallel_game_unrolls, num_actions)\n",
        "    #find best action [0 or 1 or 2 or 3] for each sub-environment\n",
        "    greedy_actions = tf.argmax(q_values, axis=1)                                                                                                   #tensor of type tf.int64 + shape(parallel_game_unrolls,1)\n",
        "    #select random action [0 or 1 or 2 or 3] for each sub-environment\n",
        "    random_actions = tf.random.uniform(shape=(self.parallel_game_unrolls,1), minval=0, maxval=self.num_possible_actions, dtype=tf.int64)           #tensor of type tf.int64 + shape(parallel_game_unrolls,1)\n",
        "    #create a boolean vector using epsilon [True (with prob = 1-epsilon), False (with prob = epsilon)]\n",
        "    epsilon_sampling = tf.random.uniform(shape=(self.parallel_game_unrolls,1), minval=0, maxval=1, dtype=tf.float32) > epsilon                     #tensor of type tf.bool + shape(parallel_game_unrolls,1)\n",
        "    #get action by applying boolean vector onto the 2 choice vectors greedy and random actions [True = greedy/vector1, False = random/vector2]\n",
        "    actions = tf.where(epsilon_sampling,greedy_actions,random_actions).numpy()                                                                     #tensor of type tf.int64 + shape(parallel_game_unrolls,1)\n",
        "    return actions\n",
        "\n",
        "  def fill_with_samples(self, dqn_network, epsilon: float):\n",
        "    \"\"\" add new samples into the ERB \"\"\"\n",
        "    states_list = []\n",
        "    actions_list = []\n",
        "    rewards_list = []\n",
        "    subsequent_states_list = []\n",
        "    terminateds_list = []\n",
        "\n",
        "    #GENERATE DATA: conduct 'unroll_steps' steps in each sub-environment\n",
        "    for i in range(self.unroll_steps):\n",
        "      #choose next action using epsilon greedy\n",
        "      actions = self.sample_epsilon_greedy(dqn_network, epsilon)\n",
        "      #actions is a 2d array (for some reason) so flatten it to 1d array\n",
        "      actions = np.array(actions).flatten()\n",
        "      #conduct action\n",
        "      next_states, rewards, terminateds, _, _ = self.envs.step(actions)\n",
        "      #save observation, action, reward, next observation\n",
        "      states_list.append(self.current_states)\n",
        "      actions_list.append(actions)\n",
        "      rewards_list.append(rewards)\n",
        "      subsequent_states_list.append(next_states)\n",
        "      terminateds_list.append(terminateds)\n",
        "      #update states\n",
        "      self.current_states = next_states\n",
        "\n",
        "    #EXTRACT INFORMATION TUPLES OF EACH SUB-ENVIRONMENT\n",
        "    def data_generator():\n",
        "      for states_batch, actions_batch, rewards_batch, subsequent_states_batch, terminateds_batch in zip(states_list, actions_list, rewards_list, subsequent_states_list, terminateds_list):\n",
        "        #for each sub-environment\n",
        "        for game_idx in range(self.parallel_game_unrolls):\n",
        "          #get state, action, reward, next state, terminated\n",
        "          state = states_batch[game_idx,:,:,:] #state is given in high, width, color channels\n",
        "          action = actions_batch[game_idx]\n",
        "          reward = rewards_batch[game_idx]\n",
        "          subsequent_state = subsequent_states_batch[game_idx]\n",
        "          terminated = terminateds_batch[game_idx]\n",
        "          yield(state,action,reward,subsequent_state,terminated)\n",
        "\n",
        "    #FEED INFORMATION TUPLES INTO A tf.DATASET\n",
        "    #give shape + data type for state,                                   action,                                  reward,                                  subsequent_state,                                 terminated\n",
        "    ds_tensor_specs = (tf.TensorSpec(shape=(210,160,3), dtype=tf.uint8), tf.TensorSpec(shape=(), dtype=tf.int32), tf.TensorSpec(shape=(), dtype=tf.int32), tf.TensorSpec(shape=(210,160,3), dtype=tf.uint8), tf.TensorSpec(shape=(), dtype=tf.bool))\n",
        "    #create tf.dataset that store all information of this step taken in all sub-environments\n",
        "    new_samples_ds = tf.data.Dataset.from_generator(data_generator, output_signature=ds_tensor_specs)\n",
        "\n",
        "    #ADD NEW DATAPOINT/SUB-DATASET TO OUR DATASET/ERB\n",
        "    #preprocess dataset\n",
        "    new_samples_ds = new_samples_ds.map(lambda state, action, reward, subsequent_state, terminated: (self.observation_preprocessing_function(state), action, reward,  self.observation_preprocessing_function(subsequent_state), terminated))\n",
        "    new_samples_ds = new_samples_ds.cache().shuffle(buffer_size=self.unroll_steps * self.parallel_game_unrolls, reshuffle_each_iteration=True)\n",
        "    #run through dataset once (without doing sth) to apply preprocessing steps and to make sure that cache is applied\n",
        "    for elem in new_samples_ds:\n",
        "      continue\n",
        "    #save dataset\n",
        "    self.data.append(new_samples_ds)\n",
        "    #get total amount of data samples (# of datasets * # of steps considered per sample * # of sub-environments)\n",
        "    datapoints_in_data = len(self.data) * self.unroll_steps * self.parallel_game_unrolls\n",
        "    #check if maximum amount of data samples is exceeded\n",
        "    if datapoints_in_data > self.max_size:\n",
        "      #delete oldest data sample to stay below max size\n",
        "      self.data.pop(0)\n",
        "\n",
        "  def create_dataset(self):\n",
        "    \"\"\" create td.data.Dataset object from the ERB \"\"\"\n",
        "    erb_dataset = tf.data.Dataset.sample_from_datasets(datasets=self.data, weights=[1/float(len(self.data)) for _ in self.data], stop_on_empty_dataset=False)\n",
        "    return erb_dataset\n"
      ],
      "metadata": {
        "id": "82qL-cvDUwDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e658dba-2b17-4b31-ef56-e921437f7350"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def observation_preprocessing_function(observation):\n",
        "  \"\"\" convert an obsevation (state) to a tensor of type tf.float32, shape (84,84,3) \"\"\"\n",
        "\n",
        "  #reduce image size from (210,160,3) to (84,84,3) for efficiency\n",
        "  observation = tf.image.resize(observation, size=(84,84), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR) #ATARI has low-resolution graphics and sharp pixelated edges, so we use Nearest Neighbor (instead of Linear Interpolation) as method to preserve those characteristics and avoid smoothing/blurring artifacts\n",
        "  #change data type from  tf.uint8 to tf.float32\n",
        "  observation = tf.cast(observation, dtype=tf.float32)\n",
        "  #zero-center, i.e. put data in the range [-1.0, 1.0]\n",
        "  observation = observation / 128.0 - 1.0\n",
        "\n",
        "  return observation\n"
      ],
      "metadata": {
        "id": "1XJIdwEvzpkB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Q-Network\n",
        "= a neural network that gets an image of Atari game state as input and returns the choosen action (the picked buttoms) as output"
      ],
      "metadata": {
        "id": "tubF_wZ178Ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dqn_network(num_actions: int):\n",
        "  \"\"\" create deep Q-network agent using functional API \"\"\"\n",
        "\n",
        "  #create input for functional tf.model api (we reduce the image size for efficiency)\n",
        "  input_layer = tf.keras.Input(shape=(84,84,3), dtype=tf.float32)\n",
        "  #convolutional layers using residual/skip connections\n",
        "  x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu')(input_layer)\n",
        "  x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu')(x)\n",
        "  #use global average pooling\n",
        "  x = tf.keras.layers.GlobalAvgPool2D()(x)\n",
        "  #apply densely connected layer\n",
        "  x = tf.keras.layers.Dense(units=64, activation='relu')(x)\n",
        "  #output layer (NO residual connection!) using no activation; creates q-values for all actions\n",
        "  y = tf.keras.layers.Dense(units=num_actions, activation='linear')(x)\n",
        "\n",
        "  model = tf.keras.Model(inputs=input_layer, outputs=y)\n",
        "  return model"
      ],
      "metadata": {
        "id": "ydDWeuvSuQV-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dqn(train_dqn_network, target_network, dataset, optimizer, discount_factor: float, num_training_steps: int, batch_size: int=256):\n",
        "  \"\"\" Train Deep Q-Network in 'num_training_steps' steps using 'optimizer' \"\"\"\n",
        "\n",
        "  #use minibatches\n",
        "  dataset = dataset.batch(batch_size).prefetch(4)\n",
        "\n",
        "  @tf.function\n",
        "  def training_step(q_targets, observations, actions):\n",
        "    \"\"\" A sub-function for a single training step\"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "      #estimate q-values for the actions that we actually took in the sample\n",
        "      q_predictions_all_actions = train_dqn_network(observations)                                   #shape of (batch_size, num_actions)\n",
        "      q_predictions = tf.gather(q_predictions_all_actions, actions, batch_dims=1)\n",
        "      #compute MSE loss\n",
        "      loss = tf.reduce_mean(tf.square(q_predictions - q_targets))\n",
        "    #calculate the gradients\n",
        "    gradients = tape.gradient(loss, train_dqn_network.trainable_variables)\n",
        "    #apply gradients on network\n",
        "    optimizer.apply_gradients(zip(gradients, train_dqn_network.trainable_variables))\n",
        "    #return loss for prediction error tracker\n",
        "    return loss\n",
        "\n",
        "  losses = []\n",
        "  q_values = []\n",
        "  for i, state_transition in enumerate(dataset):\n",
        "    state, action, reward, subsequent_state, terminated = state_transition\n",
        "    #get q-values from target network\n",
        "    all_q_values = target_network(subsequent_state)\n",
        "    #select max q-value of each sub-environment step\n",
        "    max_q_values = tf.math.reduce_max(all_q_values, axis=1)\n",
        "    #save mean of all q-values for q-value tracker\n",
        "    q_values.append(np.mean(all_q_values.numpy()))\n",
        "    #get bool-vector of whether we want to use subsequent states q-value (0 = if current state is terminated, 1 = if not terminated)\n",
        "    use_subsequent_state = tf.where(terminated, tf.zeros_like(max_q_values, dtype=tf.float32), tf.ones_like(max_q_values, dtype=tf.float32))\n",
        "    #compute q-targets\n",
        "    q_targets = reward + (discount_factor * max_q_values * use_subsequent_state)\n",
        "    #conduct a training step: update network parameters by gradient descent\n",
        "    loss = training_step(q_targets, state, action)\n",
        "    loss = loss.numpy()\n",
        "    #save loss for prediction error tracker\n",
        "    losses.append(loss)\n",
        "    #check if training done\n",
        "    if i >= num_training_steps:\n",
        "      break\n",
        "\n",
        "  #return the average over the losses for prediction error tracker + the q-values for the q-value tracker\n",
        "  return np.mean(losses), np.mean(q_values)\n"
      ],
      "metadata": {
        "id": "Wdkuyx1Q8EoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5beff21-f472-42ab-f50b-9017ada0159a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RL application"
      ],
      "metadata": {
        "id": "bdWWrcp0nQSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_q_network(test_dqn_network, environment_name: str, num_parallel_tests: int, discount_factor: float, preprocessing_function: callable, test_epsilon: float=0.05):\n",
        "  \"\"\"\n",
        "    Play n games in parallel until all are finished, to get return for return tracker\n",
        "      - This is a very inefficient way of getting the return, but quite easy.\n",
        "      - We need this function the get the return, because most samples (of x steps) do not cover the whole trajectory from start state to terminal state, but only a trajectory part.\n",
        "  \"\"\"\n",
        "  #create a vectorized environment + reset to start at initial state\n",
        "  envs = gym.vector.make(environment_name, num_parallel_tests)\n",
        "  states, _ = envs.reset()\n",
        "\n",
        "  done = False\n",
        "  timestep = 0\n",
        "  returns = np.zeros(num_parallel_tests)\n",
        "  episodes_finished = np.zeros(num_parallel_tests, dtype=bool) #np vector of shape (num_parallel_tests,1) filled woth booleans, starting with all False\n",
        "  num_possible_actions = envs.single_action_space.n\n",
        "\n",
        "  #conduct a step in each sub-environment until all sub-environments are terminated\n",
        "  while not done:\n",
        "\n",
        "    #preprocess states\n",
        "    states = preprocessing_function(states)\n",
        "    #compute q-values\n",
        "    q_values = test_dqn_network(states)\n",
        "    #get best/greedy action for each sub-environment\n",
        "    greedy_actions = tf.argmax(q_values, axis=1)                                                                             #tensor of type tf.int64 + shape (num_parallel_tests,1)\n",
        "    #get random action for each sub-environment\n",
        "    random_actions = tf.random.uniform(shape=(num_parallel_tests,1), minval=0, maxval=num_possible_actions, dtype=tf.int64)  #tensor of type tf.int64 + shape (num_parallel_tests,1)\n",
        "    #choose an action using epsilon sampling\n",
        "    epsilon_sampling = tf.random.uniform(shape=(num_parallel_tests,1), minval=0, maxval=1, dtype=tf.float32) > test_epsilon  #tensor of type tf.bool + shape (num_parallel_tests,1)\n",
        "    actions = tf.where(epsilon_sampling, greedy_actions, random_actions).numpy()                                             #tensor of type tf.int64 + shape (num_parallel_tests,1)\n",
        "    #conduct action\n",
        "    next_states, rewards, terminateds, _, _ = envs.step(actions)\n",
        "\n",
        "    #update by turning all newly terminated environments to True\n",
        "    episodes_finished = np.logical_or(episodes_finished, terminateds)\n",
        "    #update return (discounted sum of rewards of all time steps) by adding discounted reward of current time step\n",
        "    returns += ((discount_factor**timestep) * rewards) * (np.logical_not(episodes_finished).astype(np.float32))     #NOTE: We only update non-terminated sub-environments\n",
        "    #increase time step\n",
        "    timestep += 1\n",
        "    #check if all sub-environments terminated\n",
        "    done = np.all(episodes_finished)\n",
        "\n",
        "  #return the average return\n",
        "  return np.mean(returns)\n"
      ],
      "metadata": {
        "id": "QRxDwAAaKkXg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def polyak_averaging_weights(source_network, target_network, polyak_averaging_factor: float):\n",
        "  \"\"\"\n",
        "    copy the weights of a source network to the a target network in a Polyak averaging way,\n",
        "    i.e. average Source and Target network's weights in a weighted manner\n",
        "    --> If 'polyak_averaging_factor' = 0, then we copy WHOLE source network's weights.\n",
        "  \"\"\"\n",
        "\n",
        "  for target_weights, source_weights in zip(target_network.weights, source_network.weights):\n",
        "        target_weights.assign(polyak_averaging_factor * source_weights + (1 - polyak_averaging_factor) * target_weights)\n"
      ],
      "metadata": {
        "id": "a8GZ_irB8Et8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(results_df, step):\n",
        "\n",
        "  #create three subplots (one for each tracker)\n",
        "  fig, axis = plt.subplots(3,1)\n",
        "  #include the row indexes explicitly in the results.df\n",
        "  results_df['step'] = results_df.index\n",
        "  #plot the average return\n",
        "  sns.lineplot(x='step', y='average_return', data=results_df, ax=axis[0])\n",
        "  #plot the average loss\n",
        "  sns.lineplot(x='step', y='average_loss', data=results_df, ax=axis[1])\n",
        "  #plot the average q-values\n",
        "  sns.lineplot(x='step', y='average_q_values', data=results_df, ax=axis[2])\n",
        "  #create a timestring from the timestamp\n",
        "  timestring = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  #save the figure\n",
        "  plt.savefig(f'./results/{timestring}_results_step{step}.png')\n",
        "  #close the figure\n",
        "  plt.close(fig)\n"
      ],
      "metadata": {
        "id": "dKZP7SvyGJz2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dqn():\n",
        "\n",
        "  ENVIRONMENT_NAME = \"ALE/Breakout-v5\"\n",
        "  NUMBER_ACTIONS = gym.make(ENVIRONMENT_NAME).action_space.n\n",
        "  MAX_SIZE = 10000 #100000\n",
        "  PARALLEL_GAME_UNROLLS = 10 #128\n",
        "  UNROLL_STEPS = 4\n",
        "  PREFILL_STEPS = 100\n",
        "  EPSILON = 0.2\n",
        "  DISCOUNT_FACTOR = 0.995\n",
        "  POLYAK_AVERAGING_FACTOR = 0.99           #the more training we do in-between updating steps, the smaller can this factor be\n",
        "  OPTIMIZER = tf.keras.optimizers.Adam()\n",
        "  NUM_TRAINING_STEPS_PER_ITERATION = 16\n",
        "  TRAIN_BATCH_SIZE = 200 #512\n",
        "  NUM_TRAINING_ITERATIONS = 51 #50000\n",
        "  TEST_EVERY_N_STEPS = 50\n",
        "  TEST_NUM_PARALLEL_ENVIRONMENTS = 10 #128\n",
        "\n",
        "  #create experience replay buffer, DQN agent/network (= the network we train), Target network (= the network we use to calculate the Q-estimation targets)\n",
        "  erb = ExperienceReplayBuffer(MAX_SIZE, ENVIRONMENT_NAME, PARALLEL_GAME_UNROLLS, observation_preprocessing_function, UNROLL_STEPS)\n",
        "  dqn_agent = create_dqn_network(NUMBER_ACTIONS)\n",
        "  target_network = create_dqn_network(NUMBER_ACTIONS)\n",
        "\n",
        "  #initialize target network with identical weights as the DQN network\n",
        "  polyak_averaging_weights(dqn_agent, target_network, polyak_averaging_factor = 0.0)\n",
        "\n",
        "  #initialize trackers for return, prediction error, average q-values\n",
        "  return_tracker = []\n",
        "  dqn_prediction_error_tracker = []\n",
        "  avg_q_values_tracker = []\n",
        "\n",
        "  #prefill the replay buffer with wide-spread sample trajectories, by choosing totally random actions (no policy)\n",
        "  for prefill_step in range(PREFILL_STEPS):\n",
        "    erb.fill_with_samples(dqn_agent, epsilon=1.0)\n",
        "\n",
        "  #TRAIN AGENT\n",
        "  for step in range(NUM_TRAINING_ITERATIONS):\n",
        "    print('Training step: ', step)\n",
        "\n",
        "    #sample trajectories (s,a,r,s') and store them in replay buffer\n",
        "    erb.fill_with_samples(dqn_agent, EPSILON)\n",
        "    #create training dataset by selecting random samples from the replay buffer\n",
        "    dataset = erb.create_dataset()\n",
        "    #train DQN using selected samples\n",
        "    average_loss, average_q_values = train_dqn(dqn_agent, target_network, dataset, OPTIMIZER, DISCOUNT_FACTOR, NUM_TRAINING_STEPS_PER_ITERATION, TRAIN_BATCH_SIZE)\n",
        "\n",
        "    #update target network via Polyak averaging\n",
        "    polyak_averaging_weights(dqn_agent, target_network, POLYAK_AVERAGING_FACTOR)\n",
        "\n",
        "    #TEST AGENT: report return, prediction error, average q-values in N steps intervals\n",
        "    if (step % TEST_EVERY_N_STEPS == 0):\n",
        "      #test q-network to get average return\n",
        "      average_return = test_q_network(dqn_agent, ENVIRONMENT_NAME, TEST_NUM_PARALLEL_ENVIRONMENTS, DISCOUNT_FACTOR, observation_preprocessing_function)\n",
        "      #save tracked info\n",
        "      return_tracker.append(average_return)\n",
        "      dqn_prediction_error_tracker.append(average_loss)\n",
        "      avg_q_values_tracker.append(average_q_values)\n",
        "      #print average returns, losses, q-values\n",
        "      print(f\"TESTING: Average return: {average_return}, Average loss: {average_loss}, Average q-value-estimation: {average_q_values}\")\n",
        "      #put all result lists into a Pandas dataframe by transforming them into a dict first\n",
        "      results_dict = {\"average_return\": return_tracker, \"average_loss\": dqn_prediction_error_tracker, \"average q-values\": avg_q_values_tracker}\n",
        "      results_df = pd.DataFrame(results_dict)\n",
        "      #visualize the results with seaborn\n",
        "      visualize_results(results_df, step)\n",
        "      print(results_df)\n"
      ],
      "metadata": {
        "id": "TKd8igxPnmiB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test everything\n",
        "if __name__ == '__main__':\n",
        "  dqn()"
      ],
      "metadata": {
        "id": "UBkx6GJvSidb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "5f1c268e-38ce-43c5-b7a2-22b750b419db"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step:  0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d3301674bfab>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#test everything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-43605dbe8858>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m#train DQN using selected samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0maverage_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDISCOUNT_FACTOR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_TRAINING_STEPS_PER_ITERATION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_BATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m#update target network via Polyak averaging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-52a22e736191>\u001b[0m in \u001b[0;36mtrain_dqn\u001b[0;34m(train_dqn_network, target_network, dataset, optimizer, discount_factor, num_training_steps, batch_size)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0muse_subsequent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_q_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_q_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m#compute q-targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mq_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiscount_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_q_values\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0muse_subsequent_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;31m#conduct a training step: update network parameters by gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7261\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7262\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimizations\n",
        "- pre-fill training buffer before training to avoid overestimation bias\n",
        "- use separate target network, which is a delayed version of Q-network, to avoid moving target problem"
      ],
      "metadata": {
        "id": "hexxP7HZ5FX2"
      }
    }
  ]
}